{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"i0TiAJujYOQs","executionInfo":{"status":"ok","timestamp":1734352513271,"user_tz":300,"elapsed":163,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["huggingface_token = \"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\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tX61fQ26XjYV","outputId":"55c95a8f-880b-4ab1-dbf4-90118502ceaf","executionInfo":{"status":"ok","timestamp":1734352558083,"user_tz":300,"elapsed":44649,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Collecting unsloth\n","  Downloading unsloth-2024.12.4-py3-none-any.whl.metadata (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting unsloth_zoo>=2024.11.8 (from unsloth)\n","  Downloading unsloth_zoo-2024.12.1-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n","Collecting xformers>=0.0.27.post2 (from unsloth)\n","  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Collecting bitsandbytes (from unsloth)\n","  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n","Collecting triton>=3.0.0 (from unsloth)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n","Collecting tyro (from unsloth)\n","  Downloading tyro-0.9.2-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.46.3)\n","Collecting datasets>=2.16.0 (from unsloth)\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.1.1)\n","Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n","  Downloading trl-0.12.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n","Collecting protobuf<4.0.0 (from unsloth)\n","  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.26.5)\n","Collecting hf_transfer (from unsloth)\n","  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n","Collecting xxhash (from datasets>=2.16.0->unsloth)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.10)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (0.20.3)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n","Collecting cut_cross_entropy (from unsloth_zoo>=2024.11.8->unsloth)\n","  Downloading cut_cross_entropy-24.12.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth) (11.0.0)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n","Collecting shtab>=1.5.6 (from tyro->unsloth)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n","Downloading unsloth-2024.12.4-py3-none-any.whl (174 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.12.2-py3-none-any.whl (365 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth_zoo-2024.12.1-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.2-py3-none-any.whl (112 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Downloading cut_cross_entropy-24.12.2-py3-none-any.whl (22 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, triton, shtab, protobuf, hf_transfer, fsspec, dill, multiprocess, xformers, tyro, cut_cross_entropy, bitsandbytes, datasets, trl, unsloth_zoo, unsloth\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n","grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.45.0 cut_cross_entropy-24.12.2 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 hf_transfer-0.1.8 multiprocess-0.70.16 protobuf-3.20.3 shtab-1.7.1 triton-3.1.0 trl-0.12.2 tyro-0.9.2 unsloth-2024.12.4 unsloth_zoo-2024.12.1 xformers-0.0.28.post3 xxhash-3.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"df8a8c3487774c4da7abed20a4699f23"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n","ERROR: unknown command \"instal\" - maybe you meant \"install\"\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Found existing installation: unsloth 2024.12.4\n","Uninstalling unsloth-2024.12.4:\n","  Successfully uninstalled unsloth-2024.12.4\n","Collecting git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-s87pr5hh\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-s87pr5hh\n","  Resolved https://github.com/unslothai/unsloth.git to commit 85f1fa096afde5efe2fb8521d8ceec8d13a00715\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unsloth: filename=unsloth-2024.12.4-py3-none-any.whl size=173746 sha256=ceec435bf1724aa4c3e5184e1e2cb73130f22b276f9711cd65138ffa2a65c521\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-y3r3slvl/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n","Successfully built unsloth\n","Installing collected packages: unsloth\n","Successfully installed unsloth-2024.12.4\n"]}],"source":["!pip install transformers\n","!pip install torch\n","!pip install scikit-learn\n","!pip install unsloth\n","!pip install scipy\n","!pip instal numpy\n","!pip install peft\n","!pip install datasets\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"g2X08VJmXoaZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734352582249,"user_tz":300,"elapsed":24169,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"outputId":"c477fd79-d3e5-4ae0-db19-edb17098a6a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["import torch\n","import gc\n","import pandas as pd\n","import os\n","import logging\n","from unsloth import FastLanguageModel, is_bfloat16_supported\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorWithPadding, DataCollatorForSeq2Seq\n","from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n","from torch.utils.data import Dataset\n","from huggingface_hub import login\n","from google.colab import drive\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform, randint\n","import numpy as np\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KTr-FBbqXroj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734352885661,"user_tz":300,"elapsed":18761,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"outputId":"6d329cc2-7a97-4abe-de89-9826c79d2527"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Mounting Google Drive\n","drive.mount('/content/drive')\n","\n","# Defining file path base\n","base_path = '/content/drive/My Drive/clean data/'\n","\n","# Load dataframes from CSV files in Google Drive\n","dfs = {\n","    'training_crime': pd.read_csv(base_path + 'training_crime.csv')[['text', 'is_true']],\n","    'training_health': pd.read_csv(base_path + 'training_health.csv')[['text', 'is_true']],\n","    'training_politics': pd.read_csv(base_path + 'training_politics.csv')[['text', 'is_true']],\n","    'training_science': pd.read_csv(base_path + 'training_science.csv')[['text', 'is_true']],\n","    'training_social': pd.read_csv(base_path + 'training_social.csv')[['text', 'is_true']],\n","    'testing_crime': pd.read_csv(base_path + 'testing_crime.csv')[['text', 'is_true']],\n","    'testing_health': pd.read_csv(base_path + 'testing_health.csv')[['text', 'is_true']],\n","    'testing_politics': pd.read_csv(base_path + 'testing_politics.csv')[['text', 'is_true']],\n","    'testing_science': pd.read_csv(base_path + 'testing_science.csv')[['text', 'is_true']],\n","    'testing_social': pd.read_csv(base_path + 'testing_social.csv')[['text', 'is_true']]\n","}\n","\n","# Combining all training dataframes to make one merged training dataset\n","df_training = pd.concat([dfs['training_crime'], dfs['training_health'], dfs['training_politics'], dfs['training_science'],\n","                         dfs['training_social']], ignore_index=True)\n","\n","df_testing = pd.concat([dfs['testing_crime'], dfs['testing_health'], dfs['testing_politics'], dfs['testing_science'],\n","                         dfs['testing_social']], ignore_index=True)\n","# Splitting into 70% training and 30% testing\n","train_df, eval_df = train_test_split(df_training, test_size=0.3, random_state=42, stratify=None)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKBAilAfXtIH","outputId":"37b5fc01-9350-4d71-c922-2079ac5ed6fe","executionInfo":{"status":"ok","timestamp":1734359746795,"user_tz":300,"elapsed":10905,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Already have LoRA adapters! We shall skip this step.\n"]}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    #model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n","    model_name = \"/content/drive/MyDrive/Models/3-1b_model_epoch1_6\",\n","    #max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n","    token = huggingface_token\n",")\n","\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    use_gradient_checkpointing = False, # Change to 'unsloth' if you're running out of memory\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"omawbMg5XuwH","executionInfo":{"status":"ok","timestamp":1734359746795,"user_tz":300,"elapsed":20,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["class NewsDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        # Get the text and tokenize it\n","        text = self.texts[idx]\n","        tokenized_input = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n","\n","        input_ids = tokenized_input['input_ids'][0]\n","\n","        # Create labels that are the same shape as input_ids but shifted by one token\n","        labels = input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = 1000  # Mask the padding tokens if any\n","\n","        return {\n","            'input_ids': input_ids,\n","            'labels': labels\n","        }"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"-c5rCtm5XwY5","executionInfo":{"status":"ok","timestamp":1734359746795,"user_tz":300,"elapsed":20,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["train_dataset = NewsDataset(train_df['text'].tolist(), train_df['is_true'].tolist())\n","eval_dataset = NewsDataset(eval_df['text'].tolist(), eval_df['is_true'].tolist())"]},{"cell_type":"code","source":["'''\n","import logging\n","from datasets import Dataset\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding\n",")\n","from datasets import load_dataset\n","from peft import LoraConfig, get_peft_model\n","from sklearn.model_selection import ParameterGrid\n","from transformers import LlamaForSequenceClassification\n","\n","class CustomLlamaForSequenceClassification(LlamaForSequenceClassification):\n","    def forward(self, *args, num_items_in_batch=None, **kwargs):\n","        kwargs.pop(\"num_items_in_batch\", None)  # Ignore the argument\n","        return super().forward(*args, **kwargs)\n","\n","\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Load dataset\n","logger.info(\"Loading dataset...\")\n","\n","\n","# Load tokenizer and model\n","model_name = \"unsloth/Llama-3.2-1B-Instruct\"  # Replace with your model\n","logger.info(f\"Loading model and tokenizer: {model_name}\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","# Tokenize datasets\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n","\n","logger.info(\"Tokenizing dataset...\")\n","train_dataset1 = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n","eval_dataset1 = Dataset.from_pandas(eval_df).map(tokenize_function, batched=True)\n","\n","train_dataset1 = train_dataset1.rename_column(\"is_true\", \"labels\")\n","eval_dataset1 = eval_dataset1.rename_column(\"is_true\", \"labels\")\n","\n","train_dataset1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","eval_dataset1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","# Define data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# Define hyperparameter grid for search\n","param_grid = {\n","    \"r\": [4, 8, 16],  # Rank of the LoRA layers\n","    \"lora_alpha\": [16, 32, 64],  # Scaling factor\n","    \"lora_dropout\": [0.0, 0.1, 0.2],  # Dropout for LoRA layers\n","    \"learning_rate\": [1e-4, 2e-4, 5e-5],  # Learning rate\n","    \"batch_size\": [16, 32],  # Batch size\n","}\n","\n","# Grid search over hyperparameters\n","best_metric = None\n","best_config = None\n","\n","logger.info(\"Starting hyperparameter search...\")\n","for config in ParameterGrid(param_grid):\n","    logger.info(f\"Evaluating config: {config}\")\n","\n","    # Configure LoRA\n","    lora_config = LoraConfig(\n","        r=config[\"r\"],\n","        lora_alpha=config[\"lora_alpha\"],\n","        lora_dropout=config[\"lora_dropout\"],\n","        bias=\"none\",\n","        task_type=\"SEQ_CLS\",  # Task type: Sequence Classification\n","    )\n","    lora_model = get_peft_model(model, lora_config)\n","\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        num_train_epochs=3,\n","        per_device_train_batch_size=config[\"batch_size\"],\n","        per_device_eval_batch_size=config[\"batch_size\"],\n","        learning_rate=config[\"learning_rate\"],\n","        weight_decay=0.01,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        logging_dir=\"./logs\",\n","        logging_steps=50,\n","        report_to=\"none\",\n","        remove_unused_columns=False,\n","        fp16=True,  # Mixed precision training\n","    )\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=lora_model,\n","        args=training_args,\n","        train_dataset=train_dataset1,\n","        eval_dataset=eval_dataset1,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","    )\n","\n","    # Train and evaluate\n","    logger.info(\"Training...\")\n","    trainer.train()\n","\n","    logger.info(\"Evaluating...\")\n","    eval_results = trainer.evaluate()\n","    metric = eval_results[\"eval_loss\"]  # Use eval_loss or another metric for comparison\n","\n","    logger.info(f\"Metric for config {config}: {metric}\")\n","    if best_metric is None or metric < best_metric:\n","        best_metric = metric\n","        best_config = config\n","\n","logger.info(f\"Best config: {best_config} with metric: {best_metric}\")\n","'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"UHnRR14ekdcg","outputId":"1c076bda-4b8e-412e-e1fb-a8d1d1ad10f7","executionInfo":{"status":"ok","timestamp":1734359746796,"user_tz":300,"elapsed":20,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport logging\\nfrom datasets import Dataset\\nfrom transformers import (\\n    AutoModelForSequenceClassification,\\n    AutoTokenizer,\\n    Trainer,\\n    TrainingArguments,\\n    DataCollatorWithPadding\\n)\\nfrom datasets import load_dataset\\nfrom peft import LoraConfig, get_peft_model\\nfrom sklearn.model_selection import ParameterGrid\\nfrom transformers import LlamaForSequenceClassification\\n\\nclass CustomLlamaForSequenceClassification(LlamaForSequenceClassification):\\n    def forward(self, *args, num_items_in_batch=None, **kwargs):\\n        kwargs.pop(\"num_items_in_batch\", None)  # Ignore the argument\\n        return super().forward(*args, **kwargs)\\n\\n\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Load dataset\\nlogger.info(\"Loading dataset...\")\\n\\n\\n# Load tokenizer and model\\nmodel_name = \"unsloth/Llama-3.2-1B-Instruct\"  # Replace with your model\\nlogger.info(f\"Loading model and tokenizer: {model_name}\")\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=2)\\n\\n# Tokenize datasets\\ndef tokenize_function(examples):\\n    return tokenizer(examples[\"text\"], truncation=True, padding=True)\\n\\nlogger.info(\"Tokenizing dataset...\")\\ntrain_dataset1 = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\\neval_dataset1 = Dataset.from_pandas(eval_df).map(tokenize_function, batched=True)\\n\\ntrain_dataset1 = train_dataset1.rename_column(\"is_true\", \"labels\")\\neval_dataset1 = eval_dataset1.rename_column(\"is_true\", \"labels\")\\n\\ntrain_dataset1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\\neval_dataset1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\\n\\n# Define data collator\\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\n\\n# Define hyperparameter grid for search\\nparam_grid = {\\n    \"r\": [4, 8, 16],  # Rank of the LoRA layers\\n    \"lora_alpha\": [16, 32, 64],  # Scaling factor\\n    \"lora_dropout\": [0.0, 0.1, 0.2],  # Dropout for LoRA layers\\n    \"learning_rate\": [1e-4, 2e-4, 5e-5],  # Learning rate\\n    \"batch_size\": [16, 32],  # Batch size\\n}\\n\\n# Grid search over hyperparameters\\nbest_metric = None\\nbest_config = None\\n\\nlogger.info(\"Starting hyperparameter search...\")\\nfor config in ParameterGrid(param_grid):\\n    logger.info(f\"Evaluating config: {config}\")\\n\\n    # Configure LoRA\\n    lora_config = LoraConfig(\\n        r=config[\"r\"],\\n        lora_alpha=config[\"lora_alpha\"],\\n        lora_dropout=config[\"lora_dropout\"],\\n        bias=\"none\",\\n        task_type=\"SEQ_CLS\",  # Task type: Sequence Classification\\n    )\\n    lora_model = get_peft_model(model, lora_config)\\n\\n    # Define training arguments\\n    training_args = TrainingArguments(\\n        output_dir=\"./results\",\\n        num_train_epochs=3,\\n        per_device_train_batch_size=config[\"batch_size\"],\\n        per_device_eval_batch_size=config[\"batch_size\"],\\n        learning_rate=config[\"learning_rate\"],\\n        weight_decay=0.01,\\n        evaluation_strategy=\"epoch\",\\n        save_strategy=\"epoch\",\\n        logging_dir=\"./logs\",\\n        logging_steps=50,\\n        report_to=\"none\",\\n        remove_unused_columns=False,\\n        fp16=True,  # Mixed precision training\\n    )\\n\\n    # Initialize Trainer\\n    trainer = Trainer(\\n        model=lora_model,\\n        args=training_args,\\n        train_dataset=train_dataset1,\\n        eval_dataset=eval_dataset1,\\n        tokenizer=tokenizer,\\n        data_collator=data_collator,\\n    )\\n\\n    # Train and evaluate\\n    logger.info(\"Training...\")\\n    trainer.train()\\n\\n    logger.info(\"Evaluating...\")\\n    eval_results = trainer.evaluate()\\n    metric = eval_results[\"eval_loss\"]  # Use eval_loss or another metric for comparison\\n\\n    logger.info(f\"Metric for config {config}: {metric}\")\\n    if best_metric is None or metric < best_metric:\\n        best_metric = metric\\n        best_config = config\\n\\nlogger.info(f\"Best config: {best_config} with metric: {best_metric}\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qUvYx3EtXxZY","executionInfo":{"status":"ok","timestamp":1734359746796,"user_tz":300,"elapsed":6,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["# Defining the training arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=16,  # Larger batch size\n","    gradient_accumulation_steps=4,  # Fewer accumulation steps\n","    warmup_steps=5, # can change later\n","    #max_steps=60,\n","    learning_rate=2e-4,\n","    fp16=not is_bfloat16_supported(),\n","    bf16=is_bfloat16_supported(),\n","    logging_steps=1,\n","    optim=\"lion_8bit\",\n","    weight_decay=0.01,\n","    lr_scheduler_type=\"cosine\",\n","    seed=3407,\n","    output_dir=\"outputs\",\n","    report_to='none',\n","    num_train_epochs=3,\n",")\n","\n","\n","trainer = SFTTrainer( # Can try Trainer instead of SFTTrainer\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=2048,\n","    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n","    dataset_num_proc=2,\n","    packing=False,  # Set True if your sequences are short\n","    args=training_args\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYksWHnjZo2z","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b8670bda-11cc-46c3-92de-f0f435b3a4e6"},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 49,268 | Num Epochs = 3\n","O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 64 | Total steps = 2,310\n"," \"-____-\"     Number of trainable parameters = 11,272,192\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='98' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  98/2310 02:15 < 51:54, 0.71 it/s, Epoch 0.13/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.878500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.165300</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.050000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.076700</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.163300</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.026200</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.905300</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.170100</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.358400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.147800</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.117700</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.076900</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.125200</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.805600</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.123400</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.107800</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>1.188100</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.984400</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.068200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.208200</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.000600</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.056400</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.945900</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.036700</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.074500</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.122700</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.041900</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.189100</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.282900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.945300</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>1.228900</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.048200</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.086300</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.212100</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>1.014000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.872900</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.311200</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.215400</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.208500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.050100</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.138900</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.175300</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.032500</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.314000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.136900</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.938700</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>1.221200</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.083500</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.246000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.072800</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.276100</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.931500</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.085800</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.940300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.127800</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.283700</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.918800</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.066900</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.781300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.069100</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.076200</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>1.102700</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>1.141500</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.842800</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>1.023100</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>1.050700</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>1.096900</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>1.304600</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.996100</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.126700</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.996100</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.270500</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>1.228200</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.070500</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.917300</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>1.149300</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>1.021500</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.044000</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>1.288900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.104900</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>1.068900</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>1.076700</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.991400</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.971400</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>1.276800</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>1.168300</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>1.074700</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.960500</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.922200</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.494600</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>1.075300</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.458700</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>1.206900</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>1.184700</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>1.032400</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>1.239400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["#Train the model\n","trainer.train()\n","# Final evaluation\n","eval_results = trainer.evaluate()\n","print(\"Evaluation results:\", eval_results)\n","\n","model.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_9\") # Local saving\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_9\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5SUUxueXyzO"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from tqdm import tqdm  # For the progress bar\n","\n","# Prepare test data\n","texts = df_testing['text'].tolist()\n","true_labels = df_testing['is_true'].tolist()\n","\n","# Define batch size\n","batch_size = 4  # Adjust as needed\n","max_length = 2048  # Limit tokenized sequence length\n","\n","# Tokenize and predict in batches\n","predicted_labels = []\n","\n","length = len(texts)\n","num_batches = (length + batch_size - 1) // batch_size  # Calculate number of batches\n","\n","# Use torch.no_grad() to avoid gradient computation\n","with torch.no_grad():\n","    for i in tqdm(range(num_batches), desc=\"Evaluating Batches\"):\n","        # Slice batch\n","        start_idx = i * batch_size\n","        end_idx = min((i + 1) * batch_size, length)\n","        batch_texts = texts[start_idx:end_idx]\n","\n","        # Tokenize batch of texts\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length\n","        ).to(model.device)\n","\n","        # Compute logits directly\n","        logits = model(**inputs).logits\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Predict the class with the highest probability\n","        predictions = torch.argmax(probs, dim=-1).tolist()\n","        predicted_labels.extend(predictions)\n","\n","        # Clear CUDA cache to free memory\n","        torch.cuda.empty_cache()\n","\n","\n"]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import classification_report\n","\n","mlb = MultiLabelBinarizer()\n","\n","true_labelsL = [[label] for label in true_labels]\n","predicted_labelsL = [[label] for label in predicted_labels]\n","true_labelsB = mlb.fit_transform(true_labelsL)\n","predicted_labelsB = mlb.transform(predicted_labels)\n","\n","accuracy = accuracy_score(true_labelsB, predicted_labelsB)\n","'''\n","precision = precision_score(true_labelsB, predicted_labelsB, average=\"weighted\")\n","recall = recall_score(true_labelsB, predicted_labelsB, average=\"weighted\")\n","f1 = f1_score(true_labelsB, predicted_labelsB, average=\"weighted\")\n","'''\n","\n","# Print Results\n","print(f\"Accuracy: {accuracy:.2f}\")\n","'''\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n","'''\n","print(classification_report(true_labelsB, predicted_labelsB))\n","\n","\n","def reconstruct_confusion_matrix(precision, recall, accuracy, total_samples):\n","    # Calculate TP\n","    tp_fn = recall * total_samples\n","    tp = recall * tp_fn\n","\n","    # Calculate FP\n","    fp = tp * (1 / precision - 1)\n","\n","    # Calculate FN\n","    fn = tp_fn - tp\n","\n","    # Calculate TN using accuracy\n","    tn = accuracy * total_samples - tp\n","\n","    # Construct confusion matrix\n","    return [[int(tp), int(fp)], [int(fn), int(tn)]]\n","\n","\n","cm = reconstruct_confusion_matrix(precision, recall, accuracy, 139483)\n","\n","#print(cm)\n","\n","# Confusion Matrix\n","#conf_matrix = confusion_matrix(true_labelsL, predicted_labelsL)\n","\n","# Plot Confusion Matrix\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"False\", \"True\"], yticklabels=[\"False\", \"True\"])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","'''\n","# Save Results\n","with open('/content/drive/MyDrive/evals/evaluation_results_epoch9x.txt', 'w') as f:\n","    f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n","    f.write(f\"Precision: {precision:.2f}\\n\")\n","    f.write(f\"Recall: {recall:.2f}\\n\")\n","    f.write(f\"F1 Score: {f1:.2f}\\n\")\n","    f.write(f\"Confusion Matrix:\\n{cm}\\n\")\n","    '''"],"metadata":{"id":"qLFLm3CH4Gcd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}