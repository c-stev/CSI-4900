{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"i0TiAJujYOQs","executionInfo":{"status":"ok","timestamp":1734555033536,"user_tz":300,"elapsed":140,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["huggingface_token = \"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\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tX61fQ26XjYV","outputId":"c5419f49-1ac6-4bf2-db26-42a4eb8a1bd9","executionInfo":{"status":"ok","timestamp":1734555068995,"user_tz":300,"elapsed":35314,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.12.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Found existing installation: unsloth 2024.12.4\n","Uninstalling unsloth-2024.12.4:\n","  Successfully uninstalled unsloth-2024.12.4\n","Collecting git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-xg2ocueh\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-xg2ocueh\n","  Resolved https://github.com/unslothai/unsloth.git to commit 85f1fa096afde5efe2fb8521d8ceec8d13a00715\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unsloth: filename=unsloth-2024.12.4-py3-none-any.whl size=173746 sha256=8b6f8f1d18fdad6534da14255224344046bbbfc54d05590eeef99a27bdb8eedc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-922z08ie/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n","Successfully built unsloth\n","Installing collected packages: unsloth\n","Successfully installed unsloth-2024.12.4\n"]}],"source":["!pip install transformers\n","!pip install torch torchvision torchaudio --upgrade\n","!pip install torch\n","!pip install scikit-learn\n","!pip install unsloth\n","!pip install scipy\n","!pip install numpy\n","!pip install peft\n","!pip install datasets\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"g2X08VJmXoaZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734555080678,"user_tz":300,"elapsed":11686,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"outputId":"2a2e50e5-19f1-422f-d389-9fbb632bffa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["import torch\n","import gc\n","import pandas as pd\n","import os\n","import logging\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorWithPadding, DataCollatorForSeq2Seq\n","from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n","from torch.utils.data import Dataset\n","from huggingface_hub import login\n","from google.colab import drive\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform, randint\n","import numpy as np\n","from unsloth import FastLanguageModel, is_bfloat16_supported"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KTr-FBbqXroj","executionInfo":{"status":"ok","timestamp":1734555085736,"user_tz":300,"elapsed":5121,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e208b43-1a24-4321-8d26-cab51a9ade50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","is_true\n","1    27605\n","0    27605\n","Name: count, dtype: int64\n","is_true\n","0    44291\n","1    44291\n","Name: count, dtype: int64\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Mounting Google Drive\n","drive.mount('/content/drive')\n","\n","# Defining file path base\n","base_path = '/content/drive/My Drive/clean data/'\n","\n","# Load dataframes from CSV files in Google Drive\n","dfs = {\n","    'training_crime': pd.read_csv(base_path + 'training_crime.csv')[['text', 'is_true']],\n","    'training_health': pd.read_csv(base_path + 'training_health.csv')[['text', 'is_true']],\n","    'training_politics': pd.read_csv(base_path + 'training_politics.csv')[['text', 'is_true']],\n","    'training_science': pd.read_csv(base_path + 'training_science.csv')[['text', 'is_true']],\n","    'training_social': pd.read_csv(base_path + 'training_social.csv')[['text', 'is_true']],\n","    'testing_crime': pd.read_csv(base_path + 'testing_crime.csv')[['text', 'is_true']],\n","    'testing_health': pd.read_csv(base_path + 'testing_health.csv')[['text', 'is_true']],\n","    'testing_politics': pd.read_csv(base_path + 'testing_politics.csv')[['text', 'is_true']],\n","    'testing_science': pd.read_csv(base_path + 'testing_science.csv')[['text', 'is_true']],\n","    'testing_social': pd.read_csv(base_path + 'testing_social.csv')[['text', 'is_true']]\n","}\n","\n","# Combining all training dataframes to make one merged training dataset\n","df_training = pd.concat([dfs['training_crime'], dfs['training_health'], dfs['training_politics'], dfs['training_science'],\n","                         dfs['training_social']], ignore_index=True)\n","\n","# Separate the two classes\n","class_0 = df_training[df_training['is_true'] == 0]\n","class_1 = df_training[df_training['is_true'] == 1]\n","\n","# Find the count of the minority class (1s)\n","min_count = min(len(class_0), len(class_1))\n","\n","# Resample both classes to ensure balance\n","class_0_balanced = class_0.sample(n=min_count, replace=False, random_state=42)\n","class_1_balanced = class_1.sample(n=min_count, replace=False, random_state=42)\n","\n","# Combine the undersampled 0s with all the 1s\n","balanced_train_df = pd.concat([class_0_balanced, class_1_balanced]).sample(frac=1, random_state=42)\n","\n","# Verify the new balance\n","print(balanced_train_df['is_true'].value_counts())\n","\n","df_testing = pd.concat([dfs['testing_crime'], dfs['testing_health'], dfs['testing_politics'], dfs['testing_science'],\n","                         dfs['testing_social']], ignore_index=True)\n","\n","# Separate the two classes\n","class_0_test = df_testing[df_testing['is_true'] == 0]\n","class_1_test = df_testing[df_testing['is_true'] == 1]\n","\n","# Find the count of the minority class (1s)\n","min_count_test = min(len(class_0_test), len(class_1_test))\n","\n","# Resample both classes to ensure balance\n","class_0_test_balanced = class_0_test.sample(n=min_count_test, replace=False, random_state=42)\n","class_1_test_balanced = class_1_test.sample(n=min_count_test, replace=False, random_state=42)\n","\n","# Combine the undersampled 0s with all the 1s\n","balanced_test_df = pd.concat([class_0_test_balanced, class_1_test_balanced]).sample(frac=1, random_state=42)\n","\n","# Verify the new balance\n","print(balanced_test_df['is_true'].value_counts())\n","# Splitting into 70% training and 30% testing\n","train_df, eval_df = train_test_split(balanced_train_df, test_size=0.3, random_state=42, stratify=None)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JKBAilAfXtIH","executionInfo":{"status":"ok","timestamp":1734555092676,"user_tz":300,"elapsed":6958,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c05a0fc3-2da0-43a0-94d6-97fb83fb5144"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2024.12.4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n","Unsloth: Already have LoRA adapters! We shall skip this step.\n"]}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    #model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n","    model_name = \"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\",\n","    #max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n","    token = huggingface_token\n",")\n","\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    use_gradient_checkpointing = False, # Change to 'unsloth' if you're running out of memory\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"omawbMg5XuwH","executionInfo":{"status":"ok","timestamp":1734555092676,"user_tz":300,"elapsed":19,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["class NewsDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        # Get the text and tokenize it\n","        text = self.texts[idx]\n","        tokenized_input = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n","\n","        input_ids = tokenized_input['input_ids'][0]\n","\n","        # Create labels that are the same shape as input_ids but shifted by one token\n","        labels = input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = 1000  # Mask the padding tokens if any\n","\n","        return {\n","            'input_ids': input_ids,\n","            'labels': labels\n","        }"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-c5rCtm5XwY5","executionInfo":{"status":"ok","timestamp":1734555092676,"user_tz":300,"elapsed":18,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["train_dataset = NewsDataset(train_df['text'].tolist(), train_df['is_true'].tolist())\n","eval_dataset = NewsDataset(eval_df['text'].tolist(), eval_df['is_true'].tolist())"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qUvYx3EtXxZY","executionInfo":{"status":"ok","timestamp":1734555092753,"user_tz":300,"elapsed":95,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}}},"outputs":[],"source":["# Defining the training arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=16,  # Larger batch size\n","    gradient_accumulation_steps=4,  # Fewer accumulation steps\n","    warmup_steps=5, # can change later\n","    #max_steps=60,\n","    learning_rate=2e-4,\n","    fp16=not is_bfloat16_supported(),\n","    bf16=is_bfloat16_supported(),\n","    logging_steps=1,\n","    optim=\"lion_8bit\",\n","    weight_decay=0.01,\n","    lr_scheduler_type=\"cosine\",\n","    #seed=3407,\n","    output_dir=\"outputs\",\n","    report_to='none',\n","    num_train_epochs=1,\n",")\n","\n","\n","trainer = SFTTrainer( # Can try Trainer instead of SFTTrainer\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=2048,\n","    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n","    dataset_num_proc=2,\n","    packing=False,  # Set True if your sequences are short\n","    args=training_args\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hYksWHnjZo2z","executionInfo":{"status":"ok","timestamp":1734555092753,"user_tz":300,"elapsed":7,"user":{"displayName":"Mikaela Dobie","userId":"14252294992910431768"}},"colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"5a781537-4505-418b-f557-81e3e102d2c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'#Train the model\\ntrainer.train()\\n# Final evaluation\\neval_results = trainer.evaluate()\\nprint(\"Evaluation results:\", eval_results)\\n\\nmodel.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\") # Local saving\\ntokenizer.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\")'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["'''#Train the model\n","trainer.train()\n","# Final evaluation\n","eval_results = trainer.evaluate()\n","print(\"Evaluation results:\", eval_results)\n","\n","model.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\") # Local saving\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\")'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5SUUxueXyzO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"61310035-ce33-4b07-b459-5211f53e16a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Batches:  83%|████████▎ | 9217/11073 [25:43<04:48,  6.42it/s]"]}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from tqdm import tqdm  # For the progress bar\n","\n","# Prepare test data\n","texts = balanced_test_df['text'].tolist()\n","true_labels = balanced_test_df['is_true'].tolist()\n","\n","# Define batch size\n","batch_size = 8  # Adjust as needed\n","max_length = 2048  # Limit tokenized sequence length\n","\n","# Tokenize and predict in batches\n","predicted_labels = []\n","\n","length = len(texts)\n","num_batches = (length + batch_size - 1) // batch_size  # Calculate number of batches\n","\n","# Use torch.no_grad() to avoid gradient computation\n","with torch.no_grad():\n","    for i in tqdm(range(num_batches), desc=\"Evaluating Batches\"):\n","        # Slice batch\n","        start_idx = i * batch_size\n","        end_idx = min((i + 1) * batch_size, length)\n","        batch_texts = texts[start_idx:end_idx]\n","\n","        # Tokenize batch of texts\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length\n","        ).to(model.device)\n","\n","        # Compute logits directly\n","        logits = model(**inputs).logits\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Predict the class with the highest probability\n","        predictions = torch.argmax(probs, dim=-1).tolist()\n","        predicted_labels.extend(predictions)\n","\n","        # Clear CUDA cache to free memory\n","        torch.cuda.empty_cache()\n","\n","\n"]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import classification_report\n","\n","mlb = MultiLabelBinarizer()\n","\n","true_labelsL = [[label] for label in true_labels]\n","predicted_labelsL = [[label] for label in predicted_labels]\n","true_labelsB = mlb.fit_transform(true_labelsL)\n","predicted_labelsB = mlb.transform(predicted_labels)\n","\n","accuracy = accuracy_score(true_labelsB, predicted_labelsB)\n","\n","precision = precision_score(true_labelsB, predicted_labelsB, average=\"macro\")\n","recall = recall_score(true_labelsB, predicted_labelsB, average=\"macro\")\n","f1 = f1_score(true_labelsB, predicted_labelsB, average=\"macro\")\n","\n","\n","# Print Results\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","print(classification_report(true_labelsB, predicted_labelsB))\n","\n","conf_matrix = confusion_matrix(true_labelsB.argmax(axis=1), predicted_labelsB.argmax(axis=1))\n","# Confusion Matrix\n","\n","\n","# Plot Confusion Matrix\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"False\", \"True\"], yticklabels=[\"False\", \"True\"])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","'''\n"],"metadata":{"id":"qLFLm3CH4Gcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    #model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n","    model_name = \"/content/drive/MyDrive/Models/3-1b_model_epoch1_even3\",\n","    #max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n","    token = huggingface_token\n",")\n","\n","eval_results = trainer.evaluate()\n","print(\"Evaluation results:\", eval_results)"],"metadata":{"id":"Soaq6vmb8wsF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}