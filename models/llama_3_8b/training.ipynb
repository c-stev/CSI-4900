{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf56e874446935c4",
   "metadata": {
    "collapsed": false,
    "id": "bf56e874446935c4"
   },
   "source": [
    "To obtain a Hugging Face token, users must create an account on the HuggingFace website, navigate to their profile settings, and generate a new token. Once the token is acquired, users can input it in their code to seamlessly integrate Hugging Face models and functionalities into their projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1742b731ed6312a1",
   "metadata": {
    "id": "1742b731ed6312a1"
   },
   "outputs": [],
   "source": [
    "huggingface_token = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3ddfb6061c90d",
   "metadata": {
    "collapsed": false,
    "id": "28a3ddfb6061c90d"
   },
   "source": [
    "### **Installing import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc86335ae4fd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2bc86335ae4fd7c",
    "outputId": "6ed5f895-831e-4c4f-9308-10cd5e55ae0b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.11.10)\n",
      "Found existing installation: unsloth 2024.11.10\n",
      "Uninstalling unsloth-2024.11.10:\n",
      "  Successfully uninstalled unsloth-2024.11.10\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-z29ehcyp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-z29ehcyp\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8558bc92b06f9128499484ef737fa71b966ffc23\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2024.11.10-py3-none-any.whl size=166794 sha256=d6c9909e30db96c594fbd0bd27c5dfc396370a6fc95050be4614cda35ed7e989\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fzrlbo3s/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.11.10\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe274fc7a3c4e4",
   "metadata": {
    "collapsed": false,
    "id": "a6fe274fc7a3c4e4"
   },
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4386926aa26f998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4386926aa26f998",
    "outputId": "3877163c-32e0-4b62-e1d0-7a10f499c711"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import login\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b40d5e0f8802d",
   "metadata": {
    "collapsed": false,
    "id": "351b40d5e0f8802d"
   },
   "source": [
    "## **Creating the DataFrames**\n",
    "\n",
    "In this block we import all of the datasets that will be used to train and evaluate the LLM. Since the file sizes of the csvs are too large to upload to Colab directly, it is presently configured to read a file called 'clean data' that would be stored on one's Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1bf71dac-b1fd-4c47-d34d-01d942b187bb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mounting Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Defining file path base\n",
    "base_path = '/content/drive/My Drive/clean data/'\n",
    "\n",
    "# Loading dataframes from CSV files in Google Drive\n",
    "dfs = {\n",
    "    'training_crime': pd.read_csv(base_path + 'training_crime.csv')[['text', 'is_true']],\n",
    "    'training_health': pd.read_csv(base_path + 'training_health.csv')[['text', 'is_true']],\n",
    "    'training_politics': pd.read_csv(base_path + 'training_politics.csv')[['text', 'is_true']],\n",
    "    'training_science': pd.read_csv(base_path + 'training_science.csv')[['text', 'is_true']],\n",
    "    'training_social': pd.read_csv(base_path + 'training_social.csv')[['text', 'is_true']],\n",
    "}\n",
    "\n",
    "# Combining all training dataframes to make one merged training dataset\n",
    "df_training = pd.concat([dfs['training_crime'], dfs['training_health'], dfs['training_politics'], dfs['training_science'],\n",
    "                         dfs['training_social']], ignore_index=True)\n",
    "\n",
    "# Splitting into 70% training and 30% testing\n",
    "train_df, test_df = train_test_split(df_training, test_size=0.3, random_state=42, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43faf43e0b98a2d5",
   "metadata": {
    "collapsed": false,
    "id": "43faf43e0b98a2d5"
   },
   "source": [
    "## **Initializing the Model & Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Yk1FN4oOTQP0",
   "metadata": {
    "id": "Yk1FN4oOTQP0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "467c2a7c-e653-4a79-cce2-1fa465cffb17"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth 2024.11.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    #max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    token = huggingface_token\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = False, # Change to 'unsloth' if you're running out of memory\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1q_3Sdxfdvz5",
   "metadata": {
    "id": "1q_3Sdxfdvz5"
   },
   "source": [
    "## **Defining a Custom Dataset**\n",
    "\n",
    "In order to handle the text-to-token conversion, we will define a pytorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "OTamsaAMdvdI",
   "metadata": {
    "id": "OTamsaAMdvdI"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text and tokenize it\n",
    "        text = self.texts[idx]\n",
    "        tokenized_input = tokenizer(text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "\n",
    "        input_ids = tokenized_input['input_ids'][0]\n",
    "\n",
    "        # Create labels that are the same shape as input_ids but shifted by one token\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # Mask the padding tokens if any\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Splitting Up the Data**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "71a9b89b0a8fc0ac"
   },
   "id": "71a9b89b0a8fc0ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = NewsDataset(train_df['text'].tolist(), train_df['is_true'].tolist())\n",
    "test_dataset = NewsDataset(test_df['text'].tolist(), test_df['is_true'].tolist())"
   ],
   "metadata": {
    "id": "9427771d741c47a8"
   },
   "id": "9427771d741c47a8",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "IWrc2gL4eRz_",
   "metadata": {
    "id": "IWrc2gL4eRz_"
   },
   "source": [
    "## **Setting up the Trainer**\n",
    "\n",
    "In this case we will be using HuggingFace's Supervised Fine-tuning Trainer (SFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Ndjd3I1meSFT",
   "metadata": {
    "id": "Ndjd3I1meSFT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa344cd5-ddad-49ae-ab83-04c2f72202bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # Larger batch size\n",
    "    gradient_accumulation_steps=4,  # Fewer accumulation steps\n",
    "    warmup_steps=5, # can change later\n",
    "    #max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to='none',\n",
    "    #num_epochs = 1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer( # Can try Trainer instead of SFTTrainer\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Set True if your sequences are short\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "EhZGoaL6_-xS"
   },
   "id": "EhZGoaL6_-xS"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "from tqdm import tqdm  # Importing tqdm for the progress bar\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize and predict using the untrained model\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Batch processing (use a batch size)\n",
    "batch_size = 16  # Set an appropriate batch size (adjust if needed)\n",
    "num_batches = len(test_dataset) // batch_size + (1 if len(test_dataset) % batch_size != 0 else 0)\n",
    "\n",
    "# Create a progress bar\n",
    "for i in tqdm(range(num_batches), desc=\"Evaluating\", unit=\"batch\", total=num_batches):\n",
    "    # Get the batch of samples\n",
    "    batch_samples = [test_dataset[j] for j in range(i * batch_size, min((i + 1) * batch_size, len(test_dataset)))]\n",
    "\n",
    "    # Extract input_ids and labels\n",
    "    batch_input_ids = torch.stack([sample['input_ids'] for sample in batch_samples]).to(model.device)\n",
    "    batch_true_labels = [sample['labels'].cpu().numpy() for sample in batch_samples]  # True labels for the batch\n",
    "\n",
    "    # Generate model predictions (even for untrained models)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids)\n",
    "\n",
    "    # Decode the predictions and map them to labels\n",
    "    batch_predictions = outputs.logits.argmax(dim=2).cpu().numpy()\n",
    "\n",
    "    # Process each sample in the batch\n",
    "    for preds, true_labels_batch in zip(batch_predictions, batch_true_labels):\n",
    "        # Flatten lists and ignore padding token predictions (-100)\n",
    "        preds_flat = [pred for pred, label in zip(preds, true_labels_batch) if label != -100]\n",
    "        true_flat = [label for label in true_labels_batch if label != -100]\n",
    "\n",
    "        predicted_labels.extend(preds_flat)\n",
    "        true_labels.extend(true_flat)\n",
    "\n",
    "# Classification Metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "# Save plot to drive\n",
    "os.makedirs('models', exist_ok=True)\n",
    "plt.savefig('models/confusion_matrix_untrained.png')\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation metrics to a file\n",
    "eval_results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1,\n",
    "    \"confusion_matrix\": conf_matrix.tolist()\n",
    "}\n",
    "\n",
    "# Save the metrics to a text file\n",
    "with open('models/evaluation_results_untrained.txt', 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "    f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "    f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "    f.write(f\"F1 Score: {f1:.2f}\\n\")\n",
    "    f.write(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "\n",
    "print(f\"Evaluation results saved to 'models/evaluation_results_untrained.txt' and confusion matrix saved as 'models/confusion_matrix_untrained.png'\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oR9rSK-MNOHm",
    "outputId": "286ab7c8-c904-458c-cb3a-9383ac73a0ce"
   },
   "id": "oR9rSK-MNOHm",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1379/1379 [08:31<00:00,  2.69batch/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
